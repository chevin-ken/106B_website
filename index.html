<!DOCTYPE html>
<html lang="en">
  <head>
    <title>EE106B Final Project</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
    <link href="https://fonts.googleapis.com/css?family=Work+Sans:100,200,300,400,700,800" rel="stylesheet">

    <link rel="stylesheet" href="css/open-iconic-bootstrap.min.css">
    <link rel="stylesheet" href="css/animate.css">
    
    <link rel="stylesheet" href="css/owl.carousel.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">
    <link rel="stylesheet" href="css/magnific-popup.css">

    <link rel="stylesheet" href="css/aos.css">

    <link rel="stylesheet" href="css/ionicons.min.css">

    <link rel="stylesheet" href="css/bootstrap-datepicker.css">
    <link rel="stylesheet" href="css/jquery.timepicker.css">

    
    <link rel="stylesheet" href="css/flaticon.css">
    <link rel="stylesheet" href="css/icomoon.css">
    <link rel="stylesheet" href="css/style.css">
    <script
      src="https://code.jquery.com/jquery-3.3.1.js"
      integrity="sha256-2Kok7MbOyxpgUVvAk/HJ2jigOSYS2auK4Pfzbm7uH60="
      crossorigin="anonymous">
    </script>
    <script> 
      $(function(){
        $("#navbar").load("navbar.html"); 
        $("#footer").load("footer.html"); 
      });
    </script> 
  </head>
  <body>
    
    <div id="navbar"></div>
    <!-- END nav -->
    
    <section class="home-slider ftco-degree-bg">
      <div class="slider-item">
        <div class="overlay"></div>
        <div class="container">
          <div class="row slider-text align-items-center justify-content-center">
            <div class="col-md-12 ftco-animate text-center">
              <h1 class="mb-4">Grasping in the Dark:
                <strong class="typewrite" data-period="500" data-type='["3D Reconstruction in Poorly Occluded Settings"]'>
                  <span class="wrap"></span>
                </strong>
              </h1>
              <p>Group 3: Kevin Chen, Victor Ho, Amit Palekar, Jasmine Wang</p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- END slider -->

    <section class="ftco-section" id="part1">
      <div class="container">
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h2>Project Overview </h2>
          </div>
          <div class="col-md-12 text-left heading-section ftco-animate">
            <p> The ability for robots to understand and interact with their surroundings is highly dependent on their ability to perceive and gather information about the environment. 
              This usually involves many types of sensors, conveying a variety of information, and complex schemes developed to properly integrate the information supplied by these sensors. 
              However, it's not always practical to be able to obtain such a wealth of information, where resources are constrained. This project outlines a pipeline that allows for grasping in low-light conditions without the use of depth-cameras, using only RGB-image cameras.
              We aimed to create a pipeline in which we could take images of an object we intend to grasp in the dark from varying angles, and compute and execute a grasp on the object via image enhancement and mesh reconstruction.
              This is a low-cost and versatile way to grasp in the dark, which would be helpful in situations where it is difficult to produce consistent lighting for 3D reconstruction (e.g. outdoors or in search and rescue scenarios).
            </p>
          </div>
        </div>
    </section>

    <section class="ftco-section" id="part2">
      <div class="container">
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h2>Implementation and Results</h2>
          </div>
          
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h5>Setup</h5>
          </div>
          <div class="col-md-12 text-left heading-section ftco-animate">
            <p>Our pipeline consists of three stages. The first stage is responsible for converting images taken of an object we intend to grasp in the dark to enhanced versions that appear to be pictures of the object taken in bright conditions. 
              This involves using a neural network with a U-net architecture, which consists of an encoder block responsible for downsampling the image and detecting features, a decoder block responsible for upsampling features back into an image, and skip connections between the encoder and decoder block to provide concatenated features to the sparse upsampled blocks.
            The second stage involves taking these enhanced images and passing them into Colmap, a package for structure from motion and multi view stereo, to generate a 3D point cloud reconstruction of the object to grasp. We then generate a mesh from this point cloud with our custom mesh reconstruction algorithm, which is an extension of the ball-pivoting algorithm.
            The third stage is a sampling-based approach to computing grasps on a 3D mesh. We repeatedly sample two points on a mesh until we find a set of two points that is feasible for Baxter grippers to grasp, and has a high score according to our grasp metric, which determines how well a compute grasp is able to resist the force of gravity.
            This pipeline is illustrated by the flow chart below.</p>
          </div>
          </div> 
          <div class="row justify-content-center mb-4 pb-4">
            <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
              <div class="media block-10 d-block text-center">
                <div class="member-card d-flex justify-content-center">
                  <img src="images/106B_final_flowchart.png" class="img-texture-transfer">
                </div>
                <p>Implementation Diagram 
                </p>
              </div>
            </div>
          </div>


          <div class="col-md-12 text-center heading-section ftco-animate">
            <h5>Software</h5>
          </div> 
          <div class="col-md-12 text-center heading-section ftco-animate">
            <p>
              <a href="https://colab.research.google.com/drive/1PIA3zDoIfCpeQKbVkNeBgCGKQow1xKs9?usp=sharing">Google Colab notebook for dark image enhancement neural network training</a>
              <br>
              <a href="https://github.com/chevin-ken/106B_final_project">Source Code Github</a>
              <br>
              <a href="https://colmap.github.io/">Colmap</a>
            </p> 
          </div> 

          <div class="col-md-12 text-center heading-section ftco-animate">
            <h5>Report Link</h5>
          </div> 
          <div class="col-md-12 text-center heading-section ftco-animate">
            <p>
              <a href="https://drive.google.com/file/d/1024igVLYfeNeRPzuCQARXxFL0_sHkRVA/view?usp=sharing">Grasping in the Dark: 3D Reconstruction in Poorly Occluded Settings</a>
            </p>
          </div> 

          <div class="col-md-12 text-center heading-section ftco-animate">
            <h5>Demo</h5>
          </div>
          <div class="col-md-12 text-left heading-section ftco-animate">
            <p>We have included some images and videos of our pipeline in action. The first video demonstrates the manner in which we collect pictures of the pawn from varying angles in the dark. The image below shows a point cloud reconstruction of our object generated using Colmap. 
              The next video shows our mesh reconstruction algorithm in action. We first filter outliers from our pointcloud in the first few steps, and then use the ball pivoting algorithm to reconstruct the mesh from the remaining points. 
              The last video shows a grasp computed and executed successfully on the object given the mesh we generated as input.
            </p>
          </div> 
          <div class="col-md-12 text-left heading-section ftco-animate">
            <div class="row justify-content-center">
              <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
                <div class="media block-10 d-block text-center">
                  <div class="member-card d-flex justify-content-center">
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/LVjsfvBcLzs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> 
                  </div>
                  <p>Collecting images of object from different angles 
                  </p>
                </div>
              </div>
            </div>
          </div> 
          <br>
          <div class="row justify-content-center mb-4 pb-4">
            <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
              <div class="media block-10 d-block text-center">
                <div class="member-card d-flex justify-content-center">
                  <img src="images/colmap.png" class="img-part2-texture">
                </div>
                <p>Point cloud reconstruction of object generated by Colmap 
                </p>
              </div>
            </div>
          </div>
          <br>
          <div class="col-md-12 text-left heading-section ftco-animate">
            <div class="row justify-content-center">
              <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
                <div class="media block-10 d-block text-center">
                  <div class="member-card d-flex justify-content-center">
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/Rbj1EePejZs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> 
                  </div>
                  <p>Reconstructing mesh from pointcloud 
                  </p>
                </div>
              </div>
            </div>
          </div> 
          <br>
          <div class="col-md-12 text-left heading-section ftco-animate">
            <div class="row justify-content-center">
              <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
                <div class="media block-10 d-block text-center">
                  <div class="member-card d-flex justify-content-center">
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/P277wD8v41o" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>                  </div>
                  <p>Executing grasp on reconstructed mesh 
                  </p>
                </div>
              </div>
            </div>
          </div> 
        </div>
      </div>
    </section>

    <section class="ftco-section" id="part3">
      <div class="container">
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h2>Conclusion</h2>
          </div>
          <br>
          <div class="col-md-12 text-left heading-section ftco-animate">
            <p>Overall, we determined through experimentation that our our implementated pipeline fell short of our intended goal. There were a couple main reasons for this. 
              The first reason is that the dataset we collected in order to train our neural network for dark to light image enhancement was not very high quality and did not have that many dark to light image pairs.
              As a result, the neural network we trained was able to reproduce the outer geometry of our object, but could not capture a lot of the finer details and features on the object. This meant that when we tried to feed enhanced images into Colmap for image enhancement,
              there were not enough detected features to enable Colmap to generate a high quality point cloud reconstruction. Another reason is that we did not have access to a GPU when implementing this pipeline so we could not use Colmap's dense reconstruction feature.
              This meant that the point clouds generated by Colmap were sparse, resulting in reconstructed meshes that had holes and did not accurately represent the geometry of the object. However, we do believe our pipeline is designed well to potentially enable such a goal.
              This is because we tested our whole pipeline without the first stage in our pipeline by passing in images we took of our object in bright conditions directly into Colmap for reconstruction. Doing so resulted in reconstructed meshes that were not perfect,
              but were still high quality enough for our grasp computation algorithm to have a nonzero success rate in calculating and executing grasps on the object. 
            </p>
            <p>There were also difficulties we encountered related to the aforementioned problems. The first is related to collection of our dataset. In order to take pictures of our object from varying angles with known camera positions, it made the most sense to take pictures 
              with Baxter's hand camera. However, this camera is not very high quality, so the pictures taken for our training dataset were not as feature rich as if we had a higher quality camera. In addition, we did not have opportunities to collect very large datasets,
              as we needed to turn the lights on and off in the lab room in order to enable our data collection process, and so we could only do this when other groups weren't in lab, which was often at very late hours. The data collection process was also relatively time consuming, limiting
              our ability to collect very large datasets, which would have allowed us to train a much more effective model for image enhancement. Another issue also briefly mentioned above was lack of GPU on our local machines, which meant we could not use Colmap's dense point cloud reconstruction.
              This meant the meshes we generated often did not capture the finer details of the surface geometry of the objects. Finally, the lab computers ran Python2, meaning a lot of the packages we depended on were incompatible. Thus, we had to run all of our image processing on our personal laptops
              and send data back and forth being our laptops and the lab station computer, making our pipeline rather inefficient and not very autonomous.
            </p>
            <p>A few improvements we would make in future work address these issues. First, we would figure out a way to mount a higher quality camera on Baxter's arm such that the camera positions are still known but the images we collect are of higher quality.
              We would also try to find more opportunities to collect data in order to have a more robust dataset for training. In addition, we would remove our dependency on COLMAP for point cloud reconstruction and invest time in implementing our own custom solution,
              as COLMAP was designed originally for the use case where camera positions are unknown. Although COLMAP did have built in features for computing point clouds with known camera positions and scale, their results for that were not as accurate as we anticipated, so 
              it could definitely be worthwhile to implement a more accurate and dense point cloud reconstruction with known camera positions. Finally, we would definitely work on a way to make our pipeline much more autonomous such that the entire grasping procedure
              can be done with a single command, rather than requiring the need to send data back and forth between multiple machines.
            </p> 
          </div>
        </div>
      </div>
    </section>

    <section class="ftco-section" id="part4">
      <div class="container">
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h2>Team</h2>
          </div>
        </div>
        <br>
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h4>Kevin Chen</h4>
          </div>
          <div class="col-md-12 text-left heading-section ftco-animate">
            <p>Kevin is a 4th year undergraduate studying EECS at UC Berkeley. He is planning to graduate May 2022 and will be working full time as a software engineer. 
              Kevin's main focuses in computer science include machine learning, backend software development, and web development. Outside of
              computer science, Kevin enjoys playing the saxophone in the UC Berkeley Wind Ensemble, playing Teamfight Tactics, and watching the NBA and NFL (big fan of Philadelphia Eagles and 76ers).
            </p>
          </div>
          <br>
          <!-- Profile Pic -->
          <div class="row justify-content-center mb-4 pb-4">
            <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
              <div class="media block-10 d-block text-center">
                <div class="member-card d-flex justify-content-center">
                  <img src="images/kevin_profile.jpg" class="img-part2-texture">
                </div>
              </div>
            </div>
          </div>
        </div>
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h4>Amit Palekar</h4>
          </div>
          <div class="col-md-12 text-left heading-section ftco-animate">
            <p>Amit is a 3rd year undergrad majoring in Computer Science and Data Science. He will be spending the next year interning at Facebook and Rippling. His research interests lie mainly at the intersection of computer vision and control theory. 
              Outside of academics, Amit loves to play badminton, travel, and explore the city.
            </p>
          </div>
          <br>
          <!-- Profile Pic -->
          <div class="row justify-content-center mb-4 pb-4">
            <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
              <div class="media block-10 d-block text-center">
                <div class="member-card d-flex justify-content-center">
                  <img src="images/amit.png" class="img-part2-texture">
                </div>
              </div>
            </div>
          </div>
        </div>
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h4>Victor Ho</h4>
          </div>
          <div class="col-md-12 text-left heading-section ftco-animate">
            <p>Victor is a 4th year undergraduate studying EECS at UC Berkeley. He is planning to graduate May 2022 to work for a few years before looking to pursue a graduate degree. 
              Victor has experience in both electrical engineering, primarily in digital design, and computer science, focusing on backend software development.
              In his free time Victor likes playing games with friends, going hiking, and cross-country running; he has run more than 2,892 miles (longest linear distance across the contiguous US) since the start of college.
          	</p>
          </div>
          <br>
          <!-- Profile Pic -->
          <div class="row justify-content-center mb-4 pb-4">
            <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
              <div class="media block-10 d-block text-center">
                <div class="member-card d-flex justify-content-center">
                  <img src="images/victor_profile.jpg" class="img-part2-texture">
                </div>
              </div>
            </div>
          </div>
        </div>
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h4>Jasmine Wang</h4>
          </div>
          <div class="col-md-12 text-left heading-section ftco-animate">
            <p>Jasmine is a 4th year undergrad CS major at UC Berkeley. She is planning on graduating May 2022. Jasmine's interests in computer science are focused around machine learning, particularly 
              computer vision, as well as lower level programming and optimization. In her free time, Jasmine enjoys playing video games, travelling, and going on outings with friends. </p>
          </div>
          <!-- Profile Pic -->
          <div class="row justify-content-center mb-4 pb-4">
            <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
              <div class="media block-10 d-block text-center">
                <div class="member-card d-flex justify-content-center">
                  <img src="images/jasmine.jpg" class="img-part2-texture">
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <div id="footer"></div>
  
  <!-- loader -->
  <div id="ftco-loader" class="show fullscreen"><svg class="circular" width="48px" height="48px"><circle class="path-bg" cx="24" cy="24" r="22" fill="none" stroke-width="4" stroke="#eeeeee"/><circle class="path" cx="24" cy="24" r="22" fill="none" stroke-width="4" stroke-miterlimit="10" stroke="#F96D00"/></svg></div>


  <script src="js/jquery.min.js"></script>
  <script src="js/jquery-migrate-3.0.1.min.js"></script>
  <script src="js/popper.min.js"></script>
  <script src="js/bootstrap.min.js"></script>
  <script src="js/jquery.easing.1.3.js"></script>
  <script src="js/jquery.waypoints.min.js"></script>
  <script src="js/jquery.stellar.min.js"></script>
  <script src="js/owl.carousel.min.js"></script>
  <script src="js/jquery.magnific-popup.min.js"></script>
  <script src="js/aos.js"></script>
  <script src="js/jquery.animateNumber.min.js"></script>
  <script src="js/bootstrap-datepicker.js"></script>
  <script src="js/jquery.timepicker.min.js"></script>
  <script src="https://maps.googleapis.com/maps/api/js?key=AIzaSyBVWaKrjvy3MaE7SQ74_uJiULgl1JY0H2s&sensor=false"></script>
  <script src="js/google-map.js"></script>
  <script src="js/main.js"></script>
    
  </body>
</html>